# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ENKlM5qxPQLOErKvcbFBSkNGqTFH1Ru3
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from google.colab import files
uploaded = files.upload()

pf = pd.read_csv('emails.csv')
pf.head(10)

pf.shape

# NO PREPROCESSING as phake emails should include: URLs, special characters, punctuation,numbers, stopwords, capital letters,
# stemming or lemmatization, HTML tags, incorrect spelling

# INSTEAD, remove null, and perform lowercasing

pf.dropna(inplace=True)
pf['text'] = pf['text'].str.lower()

#recheck dataset quantity
pf.shape

plt.figure(figsize=(10,6))
sns.countplot(data=pf,x='label')#the orrcurrences number of each np.unique value in the column
plt.title('non-phishing vs phishing emails')
plt.xticks(ticks=[0,1],labels=['non-phishing','phishing'])
plt.ylabel('Count')
plt.show()

print(pf["label"].value_counts())#optional

# SPECIAL CHARACTER COUNTER
pf['special_characters'] = pf['text'].apply(lambda x: sum(not x.isalnum() and not x.isspace() for x in x)) #FOR EACH ROW IN "text"->CHECK CHARACTER->TRUE IF NEITHER LETTER NOR DEGIT NOR SPACE
plt.figure(figsize=(10,6))#create a new blank figure that is 10inch (width) and 6inch(height)
sns.boxplot(x='label', y='special_characters', data=pf)# tell Seaborn to use data from pf
plt.xticks([0,1], ['non-phishing', 'phishing'])
plt.xlabel('lables')
plt.ylabel('Special character count')
plt.title('number of special characters')
plt.show()

# download stopwords
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

from collections import Counter
#joining text from phishing emails -> convert to lowercasw ->split into words
phishing_text= ' '.join(pf[pf['label']==1]['text'].str.lower()).split()

# stopwords filtering conditon
stop_words = set(stopwords.words("english"))
cleaned_text = [word for word in phishing_text if word.isalpha() and word not in stop_words]# keep only alphabet words

#list common words
common_words= Counter(cleaned_text).most_common(20)
words, counts = zip(*common_words) # unzip common_words and categorize them into words and counts
plt.figure(figsize=(10,6))
sns.barplot(y=list(words),x=list(counts)) # utilsing seaborn barplot
plt.title('most common words in phishing emails')
plt.xlabel('words')
plt.ylabel('count')
plt.show()

from sklearn.model_selection import train_test_split

# 70% train + 30% remain (test and valuation)
train_text, remain_text, train_labels, remain_labels = train_test_split(
    pf['text'].tolist(),
    pf['label'].tolist(),
    test_size=0.30,
    random_state=42,
    stratify=pf['label'].tolist() # ONLY FOR LABELED DATASET
)

# 10% valuation, 20% test
val_text, test_text, val_labels, test_labels = train_test_split(
    remain_text,
    remain_labels,
    test_size=2/3,  # 20% of 30%
    random_state=42,
    stratify= remain_labels # ONLY FOR LABELED DATASET
)

!pip install transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer #can be either roBert or any BERT family members
model_name = "distilbert-base-uncased"
# instantiating DL model
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) #create model object that Trainer needs

#BERT only understand numerical input, the process should be raw text -> tokens -> numerical IDs
tokenizer = AutoTokenizer.from_pretrained(model_name) # convert texts into token IDs

#tokenize the datasets (PREPROCESSING)
MAX_LEN = 128

train_text = list(train_text)
train_labels = list(train_labels)
train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=MAX_LEN)

val_text = list(val_text)
val_labels = list(val_labels)
val_encodings = tokenizer(val_text, truncation=True, padding=True, max_length=MAX_LEN)

test_text = list(test_text)
test_labels = list(test_labels)
test_encodings = tokenizer(test_text, truncation=True, padding=True, max_length = MAX_LEN)

# this step is OPTIONAL (FOR BETTER UNDERSTANDING DATA TOKENIZATION)
# turn emails into input_ids and attention_mask
print(train_encodings['input_ids'][0]) # wordpiece token ID (Each number corresponds to a word-piece from the modelâ€™s vocabulary)
print(train_encodings['attention_mask'][0]) # tells model which tokens are padding (0) vs real words (1)
# View raw data
print(tokenizer.decode(train_encodings["input_ids"][0]))

#wrap tokenized data into a Dataset
class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):#store tokenzied input data (encodings) and labels
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx): #return the input tensors for a specific index
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype = torch.long)
        return item
    def __len__(self):#return the total number of samples
        return len(self.labels)

train_dataset = Dataset(train_encodings, train_labels)
val_dataset = Dataset(val_encodings, val_labels)
test_dataset = Dataset(test_encodings, test_labels)

# define evaluation metrics
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    accScore = accuracy_score(labels, preds)

    return {
        'accuracy': accScore, # overall correctness
        'f1': f1,             # Balance between precision & recall
        'precision': precision,## how many were actually phishing, on the of phishing labeled email (low = many false alarms)
        'recall': recall       # Out of all real phishing emails, how many did we catch (low = many phishing emails missed)
    }

#define trainning settings(before training the model)
from transformers import TrainingArguments
trainning_args = TrainingArguments( #define how the model should be trained
    output_dir='./results',          # output directory
    num_train_epochs=2,              # total number of training epochs
    per_device_train_batch_size=10,  # batch size per device during training
    learning_rate=2e-5,              #define how fast the model learns
    per_device_eval_batch_size=64,   # batch size for evaluation
    save_strategy = "epoch",          # save checkpoint after each epoch
    logging_dir='./logs',            # directory for storing logs
    report_to=["none"],                # turn off W&B while debugging
    eval_strategy="epoch",
    logging_strategy="steps",   # or "epoch"
    logging_steps=50            # ensure some train-loss logs per epoch

)

from transformers import Trainer, DataCollatorWithPadding
#Trainer simplifies teh process of training the model
trainer = Trainer(
    model = model,
    args = trainning_args,
    train_dataset = train_dataset,
    eval_dataset = val_dataset,
    compute_metrics = compute_metrics,
    tokenizer = tokenizer,
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
)

trainer.train()

eval_metrics = trainer.evaluate()     # uses val_dataset
print("Validation metrics:", eval_metrics)# shows eval_loss, accuracy, precision, recall, f1 (from compute_metrics)

# get test set prediction
pred = trainer.predict(test_dataset)
y_true = pred.label_ids
y_pred = pred.predictions.argmax(-1)

# print(confusion_matrix(y_true, y_pred))
# print(classification_report(y_true, y_pred, target_names=["Safe","Phishing"]))

test_pred = trainer.predict(test_dataset, metric_key_prefix="test")
print("Test metrics from Trainer:", test_pred.metrics)

print("\nConfusion matrix:")
print(confusion_matrix(y_true, y_pred, labels=[0,1]))
print("\nClassification report:")
print(classification_report(y_true, y_pred, labels=[0,1], target_names=["Safe","Phishing"], zero_division=0))

print("\nCore metrics:")
print("Accuracy: ", accuracy_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred, zero_division=0))
print("Recall:   ", recall_score(y_true, y_pred, zero_division=0))
print("F1 Score: ", f1_score(y_true, y_pred, zero_division=0))

import matplotlib.pyplot as plt
from collections import defaultdict


logs = trainer.state.log_history

# train loss by STEP
train_steps = [e["step"] for e in logs if "loss" in e and "step" in e]
train_loss  = [e["loss"] for e in logs if "loss" in e and "step" in e]

# eval metrics by EPOCH
eval_epochs = [int(round(e["epoch"])) for e in logs if "eval_loss" in e]
val_loss    = [e["eval_loss"] for e in logs if "eval_loss" in e]
val_acc     = [e["eval_accuracy"] for e in logs if "eval_accuracy" in e]

# Loss plot
plt.figure(figsize=(10,6))
plt.plot(train_steps, train_loss, label="Training Loss")
if val_loss:
    plt.scatter(eval_epochs, val_loss, marker="o", label="Validation Loss")
plt.xlabel("Step (train) / Epoch (eval)"); plt.ylabel("Loss")
plt.title("Training & Validation Loss"); plt.legend(); plt.tight_layout(); plt.show()

# Validation accuracy plot
if val_acc:
    plt.figure(figsize=(10,6))
    plt.plot(eval_epochs, val_acc, marker="o", label="Validation Accuracy")
    plt.xlabel("Epoch"); plt.ylabel("Accuracy")
    plt.title("Validation Accuracy"); plt.legend(); plt.tight_layout(); plt.show()

from collections import defaultdict
from math import ceil
import matplotlib.pyplot as plt

logs = trainer.state.log_history

# average training loss per epoch
epoch_sum, epoch_cnt = defaultdict(float), defaultdict(int)
last_ep = 1
for e in logs:
    if "epoch" in e:
        last_ep = max(1, int(ceil(e["epoch"])))
    if "loss" in e:
        epoch_sum[last_ep] += e["loss"]
        epoch_cnt[last_ep] += 1

train_epochs   = sorted(epoch_sum)
avg_train_loss = [epoch_sum[ep]/epoch_cnt[ep] for ep in train_epochs]

# eval (per epoch)
eval_epochs = [int(round(e["epoch"])) for e in logs if "eval_loss" in e]
val_loss    = [e["eval_loss"] for e in logs if "eval_loss" in e]
val_acc     = [e["eval_accuracy"] for e in logs if "eval_accuracy" in e]

# aligned loss plot
plt.figure(figsize=(10,6))
plt.plot(train_epochs, avg_train_loss, marker="o", label="Avg Train Loss / Epoch")
if val_loss:
    plt.plot(eval_epochs, val_loss, marker="o", label="Validation Loss")
plt.xticks(range(1, max([*train_epochs, *eval_epochs], default=1)+1))
plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Per-Epoch Train vs Validation Loss")
plt.legend(); plt.tight_layout(); plt.show()

# validation accuracy
if val_acc:
    plt.figure(figsize=(10,6))
    plt.plot(eval_epochs, val_acc, marker="o", label="Validation Accuracy")
    plt.xticks(range(1, max(eval_epochs)+1))
    plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Validation Accuracy")
    plt.legend(); plt.tight_layout(); plt.show()

args = TrainingArguments(
    output_dir='./results',
    logging_strategy="steps",     # or "epoch" (either is fine)
    logging_steps=50,             # if using "steps"
    eval_strategy="epoch",  # <-- ensures eval_* appears once per epoch
    # compute_metrics=compute_metrics,  # already set on your Trainer
)

print("train_epochs:", train_epochs)
print("avg_train_loss:", avg_train_loss)
print("eval_epochs:", eval_epochs)
print("val_loss:", val_loss)

#save the final model and tokenize
trainer.save_model('final_model')
tokenizer.save_pretrained('final_model')

# test the model
test_encoding = tokenizer(test_text, truncation=True, padding=True)
#convert to PyTorch Dataset
test_dataset = Dataset(test_encoding, test_labels)

# plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Phishing', 'Phishing'], yticklabels=['Non-Phishing', 'Phishing'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""https://www.youtube.com/watch?v=AT857mWvl0g"""
